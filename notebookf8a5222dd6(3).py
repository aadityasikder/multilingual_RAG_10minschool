# -*- coding: utf-8 -*-
"""notebookf8a5222dd6(3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Ws40UkIIV1X1WS5y7NGwRZWDTw395TZ

<a href="https://colab.research.google.com/github/aadityasikder/colab-rag-thesis/blob/main/RAG%2Bllm%20.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

!pip install -U bitsandbytes
!pip install -U transformers accelerate
!pip install transformers accelerate sentence-transformers faiss-cpu
!apt-get install -y poppler-utils
!sudo apt-get update
!sudo apt-get install tesseract-ocr-ben
!pip install fastapi uvicorn nest_asyncio pyngrok

"""# OCR Text Extraction from PDF"""

from pdf2image import convert_from_path
import pytesseract

# Path to your Bangla PDF
pdf_path = "/kaggle/input/hsc26-bangla1st-paper-pdf/HSC26-Bangla1st-Paper.pdf"

# Convert PDF pages to images
pages = convert_from_path(pdf_path, dpi=300)

text = ""
for i, page in enumerate(pages):
    img_text = pytesseract.image_to_string(page, lang="ben")
    text += img_text + "\n\n"
    print(f"‚úÖ Processed page {i+1}")

print("‚úÖ OCR completed. Total characters:", len(text))

'''
import re
import unicodedata

def clean_bangla_text(text):
    text = unicodedata.normalize("NFC", text)
    text = re.sub(r'[^\u0980-\u09FFA-Za-z0-9\s.,!?;:()-]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

text = clean_bangla_text(text)
print("‚úÖ Cleaned text length:", len(text))
'''

"""# Chunking"""

def split_text(text, chunk_size=1700, overlap=300):
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunks.append(text[i:i+chunk_size])
    return chunks

documents = split_text(text)
print("Total chunks:", len(documents))

"""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"""

'''
from huggingface_hub import login
import getpass

try:
    token = getpass.getpass("Enter your Hugging Face token (for Llama 3 access): ")
    login(token=token)
    print("--- Hugging Face Login Successful ---")
except Exception as e:
    print(f"Login failed: {e}. Ensure you have accepted Llama 3 terms on Hugging Face.")
    raise
'''

"""# Embeddings"""

from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

embedder = SentenceTransformer("intfloat/multilingual-e5-base")
doc_embeddings = embedder.encode(documents, convert_to_numpy=True)

index = faiss.IndexFlatL2(doc_embeddings.shape[1])
index.add(doc_embeddings)

"""# Loading LLM (Qwen2.5-3b) and quantization"""

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16"
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-3B-Instruct",
    device_map="auto",
    quantization_config=bnb_config,
    torch_dtype="auto"
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")

llm_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer)

def retrieve_relevant_chunks(query, k=3):
    query_embedding = embedder.encode([query])
    D, I = index.search(np.array(query_embedding), k)
    return [documents[i] for i in I[0]]

def ask_llama(query):
    context = "\n".join(retrieve_relevant_chunks(query))
    prompt = f"""Answer in the same language as the question.
Use only the following context to answer. Answer in exact one word. If answer is missing, say:
'‡¶™‡ßç‡¶∞‡¶¶‡¶§‡ßç‡¶§ ‡¶§‡¶•‡ßç‡¶Ø ‡¶•‡ßá‡¶ï‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø' for Bangla or 'Answer not found in context' for English.

Context:
{context}

Question: {query}
Answer:
"""

    result = llm_pipeline(prompt, max_new_tokens=700, do_sample=True)[0]["generated_text"]
    #return result.split("Answer:")[-1].strip() #continues with /n
    return result.split("Answer:")[-1].strip().split('\n')[0] #cuts after /n

ask_llama("‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?")

ask_llama("‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?")

ask_llama("‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?")

"""# Evaluation"""

evaluation_data = [
    {
        "query": "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡ßü ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá?",
        "expected_answer": "‡¶∂‡¶∏‡ßç‡¶§‡ßÅ‡¶®‡¶æ‡¶•",
        "gold_chunk_contains_answer": True
    },
    {
        "query": "‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?",
        "expected_answer": "‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶ï‡ßá",
        "gold_chunk_contains_answer": True
    },
    {
        "query": "‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?",
        "expected_answer": "‡ßß‡ß´ ‡¶¨‡¶õ‡¶∞",
        "gold_chunk_contains_answer": True
    }
]

"""# RAG Evaluation"""

from sklearn.metrics.pairwise import cosine_similarity

def evaluate_rag():
    total = len(evaluation_data)
    grounded = 0
    relevant = 0

    for i, item in enumerate(evaluation_data):
        query = item["query"]
        expected_answer = item["expected_answer"]
        retrieved = retrieve_relevant_chunks(query, k=4)
        context = "\n".join(retrieved)
        answer = ask_llama(query)

        print(f"\nüîπ Query: {query}")
        print(f" Expected: {expected_answer}")
        print(f" Model Answer: {answer}")

        # Groundedness: Does retrieved context contain the expected answer?
        if expected_answer in context:
            grounded += 1
            print(" Grounded: Answer is found in context.")
        else:
            print(" Not Grounded.")

        # Relevance: Does query embedding match retrieved chunk embeddings?
        query_vec = embedder.encode([query])
        retrieved_vecs = embedder.encode(retrieved)
        sims = cosine_similarity(query_vec, retrieved_vecs)[0]
        avg_sim = sims.mean()

        if avg_sim > 0.6:
            relevant += 1
            print(f" Relevant: Avg cosine similarity = {avg_sim:.2f}")
        else:
            print(f" Low relevance: Avg cosine similarity = {avg_sim:.2f}")

    print(f"\n Evaluation Summary:")
    print(f"Grounded Answers: {grounded}/{total}")
    print(f"Relevant Retrievals: {relevant}/{total}")

evaluate_rag()

"""# Short-Term Memory"""

chat_history = []

def ask_llama(query):
#last 3 questions saved
    history_text = ""
    for q, a in chat_history[-3:]:
        history_text += f"User: {q}\nAssistant: {a}\n"

    # Retrieve long-term memory from FAISS
    context = "\n".join(retrieve_relevant_chunks(query))

    prompt = f"""
You are a helpful assistant. Answer in the same language as the question.
Here is the recent conversation:
{history_text}

Use only the context to answer. If the answer is not present, say:
Bangla ‚Üí "‡¶™‡ßç‡¶∞‡¶¶‡¶§‡ßç‡¶§ ‡¶§‡¶•‡ßç‡¶Ø ‡¶•‡ßá‡¶ï‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡ßü‡¶®‡¶ø"
English ‚Üí "Answer not found in context".

Context:
{context}

Question: {query}
Answer:
"""
    result = llm_pipeline(prompt, max_new_tokens=500, do_sample=False)[0]["generated_text"]

    final_answer = result.split("Answer:")[-1].strip().split('\n')[0]

    # Save this Q&A in short-term memory
    chat_history.append((query, final_answer))

    return final_answer

print(ask_llama("‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶¨‡¶®‡ßç‡¶ß‡ßÅ ‡¶ï‡ßá?"))
print(ask_llama("Was his marriage completed?"))

"""# Fast API (REST)"""

from fastapi import FastAPI
from pydantic import BaseModel
import nest_asyncio
import uvicorn

app = FastAPI(title="Bangla-English RAG API")

class QueryRequest(BaseModel):
    query: str

@app.post("/ask")
async def ask_question(req: QueryRequest):
    answer = ask_llama(req.query)  # Uses your RAG pipeline with short-term memory
    return {"question": req.query, "answer": answer}

# Required for running in Jupyter/Colab
nest_asyncio.apply()

!ngrok config add-authtoken 30QADLPmxJpcXxPsRxprTAf8zTq_37Z1Ep8eU9NoA14C7ESrc

from pyngrok import ngrok

public_url = ngrok.connect(8000)
print(f"‚úÖ API is live at: {public_url}/docs")

# Start FastAPI server
uvicorn.run(app, host="0.0.0.0", port=8000)